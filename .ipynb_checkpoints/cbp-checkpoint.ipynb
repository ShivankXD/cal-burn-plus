{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11196eba-4132-4f65-ae8d-c434c308c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff63479-15ef-4838-a125-3224c2db2df9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'calories.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_calories \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcalories.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_exercise \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexercise_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'calories.csv'"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_calories = pd.read_csv(\"calories.csv\")\n",
    "df_exercise = pd.read_csv(\"exercise_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeed0f-0b3f-4d32-ad7b-4e358e9f01ba",
   "metadata": {},
   "source": [
    " Imported Pandas and the required  training datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773ba98-6dc3-427e-8abc-97f2921c86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_calories.head())\n",
    "print(df_exercise.head())   #Testing if dataset is getting displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef6d9f-48a8-42ea-985a-8e63def56af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_calories.info())\n",
    "print(df_exercise.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c530d03-d4c6-4827-bd9b-c3972e91fb59",
   "metadata": {},
   "source": [
    "### Data cleaning and Handling missing values \n",
    "Removing rows which might have missing values . \n",
    "Alternately missing values can be filled with the mean, median, or mode, depending on the data type: df.fillna(df.mean(), inplace=True)  # For numerical columns\n",
    "In this scenario , i am dropping the missing values . \n",
    "Also , Z-Score can be used to remove any outliers (Values which might the model slow or give inaccurate answers ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76272761-671d-4048-a161-fcb535588266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exercise.dropna(inplace=True)\n",
    "df_calories.dropna(inplace=True)\n",
    "df_exercise.drop_duplicates(inplace=True) # Removing Duplicate Values\n",
    "df_calories.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845018d-1b31-4a42-a42a-3091451a28e7",
   "metadata": {},
   "source": [
    "### Starting the Merge procedure for both the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32777f-04d1-40ff-a4ab-32c7dd92d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weight columns in df_exercise to strings for proper matching\n",
    "weight_columns = ['130 lb', '155 lb', '180 lb', '205 lb']\n",
    "df_exercise.rename(columns={col: col.replace(\" \", \"\") for col in weight_columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274db6f3-122a-497e-868b-0f1cf68f955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to map weight to category\n",
    "def map_weight_to_column(weight):\n",
    "    if weight <= 130:\n",
    "        return '130lb'\n",
    "    elif weight <= 155:\n",
    "        return '155lb'\n",
    "    elif weight <= 180:\n",
    "        return '180lb'\n",
    "    elif weight <= 205:\n",
    "        return '205lb'\n",
    "    else:\n",
    "        return '205lb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29369b-5cd3-4913-86af-e2769df33820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping\n",
    "df_calories['Weight_Category'] = df_calories['Weight'].apply(map_weight_to_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81535e59-3716-4cc1-90b2-1477beef3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign an Activity to each user , since activities are not yet assigned to users from first dataset \n",
    "df_calories['Activity'] = np.random.choice(df_exercise['Activity'], size=len(df_calories), replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ca293-264f-46d1-8802-354fbda937f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calories['Weight_Category'] = df_calories['Weight_Category'].astype(str)\n",
    "df_exercise.columns = df_exercise.columns.astype(str)  # Convert all column names to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82115dca-08d7-4de2-b4de-fcfe911b8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names in df_exercise to strings\n",
    "df_exercise.columns = df_exercise.columns.astype(str)\n",
    "\n",
    "# Merge only on 'Activity'\n",
    "df_merged = df_calories.merge(df_exercise, how='left', on='Activity')\n",
    "\n",
    "# Select the correct column dynamically using .apply() and .get()\n",
    "df_merged['Calories_Burned'] = df_merged.apply(lambda row: row.get(str(row['Weight_Category']), None), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab4eca-1ef1-4ea4-a186-4fd1f48ab750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_merged) # Checking if merged  data getting displayed or not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750e644-37aa-437f-9f2c-7e1767dacc7a",
   "metadata": {},
   "source": [
    "### Starting with feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521904be-5486-4928-a461-505489b14c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Total_Calories_Burned'] = df_merged['Calories_Burned'] * df_merged['Duration'] #Calculating total calories burned \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb06d95-a51d-4541-ac9c-b3029b398531",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df_merged[df_merged['Calories_Burned'].isnull()]\n",
    "print(missing_data)\n",
    "# checking missing values in new merged file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a440341-0374-41f9-8455-a51a8b7a3bdd",
   "metadata": {},
   "source": [
    "Missing values can either be dropped or Replaced by other values .  I am Replacing them with Mean(Average) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8952f-2ed7-4c20-b15a-aa37c2dc2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.groupby('Activity')['Calories_Burned'].sum().plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Calories Burned per Activity')\n",
    "plt.ylabel('Calories Burned')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce51842-7467-4d55-9d73-c5e568dda0b1",
   "metadata": {},
   "source": [
    "### Visualizing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63ce31-b7b6-4165-a79a-cbc5939cf8bb",
   "metadata": {},
   "source": [
    "status = success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcdd22-5955-41ed-a78f-7f8a9fda73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e068f5-da96-47ad-847c-9b512fcd5e93",
   "metadata": {},
   "source": [
    "### Visualizing some more data to gain insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a60c19-b64c-4223-8d92-74aad4d29c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plotting\n",
    "ax.scatter(df_merged['Duration'], df_merged['Calories_Burned'], df_merged['Weight'], \n",
    "           c=df_merged['Calories_Burned'], cmap='coolwarm', s=50)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Duration (mins)')\n",
    "ax.set_ylabel('Calories Burned')\n",
    "ax.set_zlabel('Weight (kg)')\n",
    "ax.set_title('3D Scatter Plot: Duration vs Calories vs Weight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f57f49-a7c8-4e51-af79-2a8b20276deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data\n",
    "grouped = df_merged.groupby(['Activity', 'Weight_Category'])['Calories_Burned'].sum().reset_index()\n",
    "\n",
    "# Map categories to numbers\n",
    "activities = grouped['Activity'].unique()\n",
    "weights = grouped['Weight_Category'].unique()\n",
    "x_pos = np.arange(len(activities))\n",
    "y_pos = np.arange(len(weights))\n",
    "x, y = np.meshgrid(x_pos, y_pos)\n",
    "\n",
    "# Reshape data\n",
    "z = np.zeros_like(x)\n",
    "dx = dy = 0.5\n",
    "dz = grouped.pivot(index='Weight_Category', columns='Activity', values='Calories_Burned').fillna(0).values.flatten()\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.bar3d(x.flatten(), y.flatten(), z.flatten(), dx, dy, dz, shade=True, cmap='viridis')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks(x_pos + dx / 2)\n",
    "ax.set_xticklabels(activities, rotation=45)\n",
    "ax.set_yticks(y_pos + dy / 2)\n",
    "ax.set_yticklabels(weights)\n",
    "ax.set_zlabel('Calories Burned')\n",
    "\n",
    "plt.title('3D Bar Plot: Calories Burned by Activity & Weight Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226181c-4595-49f3-ac9c-f30419990d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.scatter_3d(df_merged, \n",
    "                    x='Duration', \n",
    "                    y='Calories_Burned', \n",
    "                    z='Weight', \n",
    "                    color='Activity', \n",
    "                    size='Calories_Burned',\n",
    "                    title='Interactive 3D Scatter Plot')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28c360-3cf5-4c77-a622-458290e3a7c8",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "    Involves,\n",
    "1. Encoding categorical variables ,\n",
    "2. Feature scaling (KNN or SVM etc)  ,\n",
    "3. Spliting the dataset(training set and testing set ) \n",
    "### Encoding Categorical Variables \n",
    "Since we r  having columns like 'Activity' and 'Weight_Category', we'll apply encoding techniques: (so that it can be readily used by our ML model later ) \n",
    "- Label Encoding (for ordinal categories, if any).\n",
    "- One-Hot Encoding (for nominal categories like 'Activity')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098bd68-f72a-4b57-a574-dd1e89550052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for categorical columns\n",
    "categorical_cols = df_merged.select_dtypes(include=['object']).columns\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9799a0-47b1-4303-b332-ca1ac36a3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying One-hot encoding\n",
    "df_encoded = pd.get_dummies(df_merged, columns=['Activity', 'Weight_Category'], drop_first=True) #\n",
    "\n",
    "# Check the new dataframe\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f62621-6ba8-4a68-b3c3-b6d75823b137",
   "metadata": {},
   "source": [
    "- Above , drop_first=True helps avoid the dummy variable trap (multicollinearity).\n",
    "- Multicollinearity occurs when two or more explanatory variables in a multiple regression model are highly linearly related. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd74708-48dc-4189-ba6b-491d2a3a091f",
   "metadata": {},
   "source": [
    "### Splitting the data into Training set and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f10af-bc82-46cc-b73c-0aed992fa5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop('Calories_Burned', axis=1)\n",
    "y = df_encoded['Calories_Burned']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6bad0-5577-4d60-b436-53622037934e",
   "metadata": {},
   "source": [
    "### Model Selection : Using Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ced563-631e-4a02-a528-b57d78ee0f80",
   "metadata": {},
   "source": [
    "Applying linear regression \n",
    "\n",
    "**Fixing non-numeric columns so that Linear Regression can run properly**\n",
    "**Since Linear Regression requires numeric data to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79698b41-8650-4f43-9975-83090003a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Apply one-hot encoding to non-numeric columns\n",
    "X_train = pd.get_dummies(X_train, columns=non_numeric_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=non_numeric_cols, drop_first=True)\n",
    "\n",
    "# Align train and test sets to have the same columns\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d6aff-00bd-44a4-aa78-171468b6ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee53690-dbbb-4ae6-aae3-f4f3a2f45873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6, label='Actual vs Predicted')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Ideal Line')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Calories Burned')\n",
    "plt.ylabel('Predicted Calories Burned')\n",
    "plt.title('Actual vs Predicted Calories Burned (Linear Regression)')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bbd42-75b5-452f-8770-12dc47ca5010",
   "metadata": {},
   "source": [
    "**Additional Visualizations**\n",
    "  1. Residual Plot : helps diagnose model performance\n",
    "  2. Feature Importance Plot :  shows the contribution of each feature to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f455e-0746-4db3-a63c-910dba3eaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, color='green', alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Calories Burned')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764d5c9-9ece-4b96-becf-c327bc0fdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = lr_model.coef_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, coefficients, color='purple')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Linear Regression Coefficients)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290f4dd-2f3a-4a51-9a01-dcc18340b2a4",
   "metadata": {},
   "source": [
    "**As can be seen above , my feature importance plot isn't showing proper values or the coffecients seems off**\n",
    "Reasons might be Scaling Issues or multicollinearity . This case feels as a bit of scaling problem . \n",
    "scaling issues : Linear regression coefficients are sensitive to the scale of the features. If some features are on a much larger scale than others, their coefficients might dominate the plot.\n",
    "Solution will be to scale my features before fitting the model (e.g., using StandardScaler or MinMaxScaler).\n",
    "\n",
    "**Using StandardScaler** To Scale properly and visualize the feature plot correctly \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e2b82-2352-4d19-b0e0-289cad745f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the linear regression model on scaled data\n",
    "lr_model_scaled = LinearRegression()\n",
    "lr_model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get coefficients and feature names\n",
    "coefficients = lr_model_scaled.coef_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, coefficients, color='purple')\n",
    "plt.xlabel('Coefficient Value (Scaled Features)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Linear Regression Coefficients)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708770d1-1354-45ed-979b-b742c10698eb",
   "metadata": {},
   "source": [
    "**SUCCESS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843d8b5-f15b-4c39-a018-8678611d0062",
   "metadata": {},
   "source": [
    "Now We have to Evaluate our model to check how well it is performing \n",
    "Evaluation of model here can be done by \n",
    "1. MSE\n",
    "2. RMSE\n",
    "3. R²\n",
    " \n",
    "**Evaluating the model using Mean Squared error on the Test Set**  \n",
    "Calculating square root of MSE to get RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153897a-dc69-4dd3-81e2-fa89e7ce5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479505a8-0b96-497e-b391-8d181764ce1f",
   "metadata": {},
   "source": [
    "**Detailed Evaluation and Visualization using all three: plus residual analysis is also used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58fa042-d2d8-49ee-8d57-39f0952175d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# R-squared (R²) Score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plotting Residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Analysis')\n",
    "plt.show()\n",
    "\n",
    "# Printing the Metrics\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R² Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb3ef8-08b4-4a23-9a43-4b3fb806d5f6",
   "metadata": {},
   "source": [
    "**Optionally we can Check for outliers if any and re-evaluate the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d61ad1-d12a-4e4e-8f65-447b2e58a690",
   "metadata": {},
   "source": [
    "Steps to Fix Outliers and Re-evaluate the Model :\n",
    "1. **Detect Outliers**\n",
    "Use statistical methods like Z-score, IQR (Interquartile Range), or visualization techniques like boxplots to detect outliers.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "#### Calculate Z-scores for the features\n",
    "z_scores = zscore(X_train)\n",
    "\n",
    "#### Define a threshold (e.g., 3 or -3)\n",
    "threshold = 3\n",
    "outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "\n",
    "#### Print number of outliers\n",
    "print(\"Number of outliers:\", outliers.sum())\n",
    "\n",
    "## Alternatively, use the IQR method:\n",
    "Q1 = X_train.quantile(0.25)\n",
    "Q3 = X_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#### Define outliers\n",
    "outliers = ((X_train < (Q1 - 1.5 * IQR)) | (X_train > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "print(\"Number of outliers:\", outliers.sum())\n",
    "\n",
    "2. **Handle Outliers**\n",
    "We can either remove outliers or transform them (e.g., using log transformation or capping).\n",
    "\n",
    "Option 1: Remove Outliers\n",
    "X_train_no_outliers = X_train[~outliers]\n",
    "\n",
    "y_train_no_outliers = y_train[~outliers]\n",
    "\n",
    "Option 2: Transform Outliers\n",
    "Cap outliers to a specific value (e.g., 95th percentile).\n",
    "upper_limit = X_train.quantile(0.95)\n",
    "lower_limit = X_train.quantile(0.05)\n",
    "X_train_capped = X_train.clip(lower_limit, upper_limit, axis=1)\n",
    "\n",
    "\n",
    "3. **Re-train the Model**\n",
    "Train your linear regression model on the cleaned dataset (without outliers or with transformed outliers).\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model_no_outliers = LinearRegression()\n",
    "lr_model_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)\n",
    "\n",
    "4. **Re-evaluate the Model**\n",
    "Evaluate the model on the test set (or use cross-validation) and compare the metrics with the previous results.\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#### Predict on the test set\n",
    "y_pred_no_outliers = lr_model_no_outliers.predict(X_test)\n",
    "#### Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred_no_outliers)\n",
    "r2 = r2_score(y_test, y_pred_no_outliers)\n",
    "print(\"Mean Squared Error (after fixing outliers):\", mse)\n",
    "print(\"R² Score (after fixing outliers):\", r2)\n",
    "\n",
    "\n",
    "**Expected Changes in Metrics**\n",
    "\n",
    "R² Score: If outliers were negatively affecting the model, fixing them may improve the R² score (closer to 1).\n",
    "\n",
    "Mean Squared Error (MSE): Outliers often increase MSE. Fixing them may reduce the MSE.\n",
    "\n",
    "Mean Absolute Error (MAE): Similar to MSE, MAE may also decrease after handling outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204f710-29a5-4630-9388-42be993d9e6c",
   "metadata": {},
   "source": [
    "Validating the model on **new** data\n",
    "\n",
    "- We can test the model on new dataset(if available) to ensure it generalizes  well\n",
    "- Since i can't find any new dataset for this , we can use  **cross-validation** to further validate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa73dc7-a064-4abc-bde5-913b9dd0a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation R² Scores:\", scores)\n",
    "print(\"Mean R²:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea383722-37d3-4182-9301-289db6035bde",
   "metadata": {},
   "source": [
    "**As can be seen above we got good and pretty much decent scores now**\n",
    "since scores seem to be too perfect , this indicates\n",
    "\n",
    "**Potential Overfitting:**\n",
    "If your model is achieving near-perfect scores, it might be overfitting the training data. Overfitting occurs when the model learns noise or specific patterns in the training data that do not generalize to new data.\n",
    "\n",
    "**Data Leakage:**\n",
    "Ensure there is no data leakage (e.g., the target variable or related information accidentally being included in the features).\n",
    "\n",
    "**Simple Dataset:**\n",
    "If your dataset is very simple or has a strong linear relationship between features and the target, such high R² scores might be realistic.\n",
    "\n",
    "Optionally we can check this , but let it slide for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d5646-80db-4174-8cfc-883831e7db83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
